os_valid_distributions:
  - RedHat
  - CentOS
  - Rocky
  - OracleLinux
  - Ubuntu
  - Debian
  - AlmaLinux

os_minimum_versions:
  RedHat: 8
  CentOS: 8
  Rocky: 8.4
  AlmaLinux: 8.3
  OracleLinux: 8
  Ubuntu: 22.04
  Debian: 11

### main --------------------------
# ---------------------------------------------------------------------
# Proxy variables (optional) for download packages using a proxy server
proxy_env: {} # yamllint disable rule:braces
#  http_proxy: http://10.128.64.9:3128
#  https_proxy: http://10.128.64.9:3128
# ---------------------------------------------------------------------

# Cluster variables
cluster_vip: '' # IP address for client access to the databases in the cluster (optional).
vip_interface: '{{ ansible_default_ipv4.interface }}' # interface name (e.g., "ens32").
# Note: VIP-based solutions such as keepalived or vip-manager may not function correctly in cloud environments.

patroni_cluster_name: 'githarbor' # the cluster name (must be unique for each cluster)

patroni_superuser_username: 'postgres'
patroni_superuser_password: 'q8PDXc4DBkwyvEPDwM9PpsnOD4Mdt3ype' # Please specify a password. If not defined, will be generated automatically during deployment.
patroni_replication_username: 'replicator'
patroni_replication_password: '' # Please specify a password. If not defined, will be generated automatically during deployment.

synchronous_mode: true # or 'true' for enable synchronous database replication
synchronous_mode_strict: true # if 'true' then block all client writes to the master, when a synchronous replica is not available
synchronous_node_count: 1 # number of synchronous standby databases

# Load Balancing
with_haproxy_load_balancing: true # or 'true' if you want to install and configure the load-balancing
haproxy_listen_port:
  master: 5000
  replicas: 5001
  replicas_sync: 5002
  replicas_async: 5003
  # The following ('_direct') ports are used for direct connections to the PostgreSQL database,
  # bypassing the PgBouncer connection pool (if 'pgbouncer_install' is 'true').
  # Uncomment the relevant lines if you need to set up direct connections.
  #  master_direct: 6000
  #  replicas_direct: 6001
  #  replicas_sync_direct: 6002
  #  replicas_async_direct: 6003
  stats: 7000
haproxy_maxconn:
  global: 100000
  master: 10000
  replica: 10000
haproxy_timeout:
  client: '60m'
  server: '60m'
# Optionally declare log format for haproxy.
# Uncomment following lines (and remove extra space in front of variable definition) for JSON structured log format.
# haproxy_log_format: "{
#  \"pid\":%pid,\
#  \"haproxy_frontend_type\":\"tcp\",\
#  \"haproxy_process_concurrent_connections\":%ac,\
#  \"haproxy_frontend_concurrent_connections\":%fc,\
#  \"haproxy_backend_concurrent_connections\":%bc,\
#  \"haproxy_server_concurrent_connections\":%sc,\
#  \"haproxy_backend_queue\":%bq,\
#  \"haproxy_server_queue\":%sq,\
#  \"haproxy_queue_wait_time\":%Tw,\
#  \"haproxy_server_wait_time\":%Tc,\
#  \"response_time\":%Td,\
#  \"session_duration\":%Tt,\
#  \"request_termination_state\":\"%tsc\",\
#  \"haproxy_server_connection_retries\":%rc,\
#  \"remote_addr\":\"%ci\",\
#  \"remote_port\":%cp,\
#  \"frontend_addr\":\"%fi\",\
#  \"frontend_port\":%fp,\
#  \"frontend_ssl_version\":\"%sslv\",\
#  \"frontend_ssl_ciphers\":\"%sslc\",\
#  \"haproxy_frontend_name\":\"%f\",\
#  \"haproxy_backend_name\":\"%b\",\
#  \"haproxy_server_name\":\"%s\",\
#  \"response_size\":%B,\
#  \"request_size\":%U\
#  }"

# keepalived (if 'cluster_vip' is specified and 'with_haproxy_load_balancing' is 'true')
keepalived_virtual_router_id: "{{ cluster_vip.split('.')[3] | int }}" # The last octet of 'cluster_vip' IP address is used by default.
# virtual_router_id - must be unique in the network (available values are 0..255).

# vip-manager (if 'cluster_vip' is specified and 'with_haproxy_load_balancing' is 'false')
vip_manager_version: '3.0.0' # version to install
vip_manager_conf: '/etc/patroni/vip-manager.yml'
vip_manager_interval: '1000' # time (in milliseconds) after which vip-manager wakes up and checks if it needs to register or release ip addresses.
vip_manager_iface: '{{ vip_interface }}' # interface to which the virtual ip will be added
vip_manager_ip: '{{ cluster_vip }}' # the virtual ip address to manage
vip_manager_mask: '24' # netmask for the virtual ip
vip_manager_dcs_type: '{{ dcs_type }}' # etcd, consul or patroni

# DCS (Distributed Consensus Store)
dcs_exists: false # or 'true' if you don't want to deploy a new etcd cluster
dcs_type: 'etcd' # or 'consul'

# if dcs_type: "etcd" and dcs_exists: false
etcd_version: '3.5.17' # version for deploy etcd cluster
etcd_data_dir: '/var/lib/etcd'
etcd_cluster_name: 'etcd-{{ patroni_cluster_name }}' # ETCD_INITIAL_CLUSTER_TOKEN

# if dcs_type: "etcd" and dcs_exists: true
patroni_etcd_hosts: [] # list of servers of an existing etcd cluster
#  - { host: "10.128.64.140", port: "2379" }
#  - { host: "10.128.64.142", port: "2379" }
#  - { host: "10.128.64.143", port: "2379" }
patroni_etcd_namespace: 'service' # (optional) etcd namespace (prefix)
patroni_etcd_username: '' # (optional) username for etcd authentication
patroni_etcd_password: '' # (optional) password for etcd authentication
patroni_etcd_protocol: "{{ 'https' if tls_cert_generate | bool else 'http' }}"

# more options you can specify in the roles/patroni/templates/patroni.yml.j2
# https://patroni.readthedocs.io/en/latest/yaml_configuration.html#etcd
# https://patroni.readthedocs.io/en/latest/yaml_configuration.html#consul

# if dcs_type: "consul"
consul_version: 'latest' # or a specific version (e.q., '1.18.2') if 'consul_install_from_repo' is 'false'
consul_install_from_repo: true # specify 'false' only if patroni_installation_method: "pip" is used
consul_config_path: '/etc/consul'
consul_configd_path: '{{ consul_config_path }}/conf.d'
consul_data_path: '/var/lib/consul'
consul_domain: 'consul' # Consul domain name
consul_datacenter: 'dc1' # Datacenter label (can be specified for each host in the inventory)
consul_disable_update_check: true # Disables automatic checking for security bulletins and new version releases
consul_enable_script_checks: true # This controls whether health checks that execute scripts are enabled on this agent
consul_enable_local_script_checks: true # Enable them when they are defined in the local configuration files
consul_ui: false # Enable the consul UI?
consul_syslog_enable: true # Enable logging to syslog
consul_iface: '{{ ansible_default_ipv4.interface }}' # specify the interface name with a Private IP (ex. "enp7s0")
consul_client_address: '127.0.0.1' # Client address. Affects DNS, HTTP, HTTPS, and gRPC client interfaces.
# TLS
# You can enable TLS encryption by dropping a CA certificate, server certificate, and server key in roles/consul/files/
consul_tls_enable: false
consul_tls_ca_crt: 'ca.crt'
consul_tls_server_crt: 'server.crt'
consul_tls_server_key: 'server.key'
# DNS
consul_recursors: [] # List of upstream DNS servers
consul_dnsmasq_enable: true # Enable DNS forwarding with Dnsmasq
consul_dnsmasq_cache: 0 # dnsmasq cache-size (0 - disable caching)
consul_dnsmasq_servers: '{{ nameservers }}' # Upstream DNS servers used by dnsmasq

# if dcs_type: "consul" and dcs_exists: true
consul_join: [] # List of LAN servers of an existing consul cluster, to join.
# - "10.128.64.140"
# - "10.128.64.142"
# - "10.128.64.143"

# https://developer.hashicorp.com/consul/docs/discovery/services
consul_services:
  - name: '{{ patroni_cluster_name }}'
    id: '{{ patroni_cluster_name }}-master'
    tags: ['master', 'primary']
    port: '{{ pgbouncer_listen_port }}' # or "{{ postgresql_port }}" if pgbouncer_install: false
    checks:
      - { http: 'http://{{ inventory_hostname }}:{{ patroni_restapi_port }}/primary', interval: '2s' }
      - { args: ['systemctl', 'status', 'pgbouncer'], interval: '5s' } # comment out this check if pgbouncer_install: false
  - name: '{{ patroni_cluster_name }}'
    id: '{{ patroni_cluster_name }}-replica'
    tags: ['replica']
    port: '{{ pgbouncer_listen_port }}'
    checks:
      - {
          http: 'http://{{ inventory_hostname }}:{{ patroni_restapi_port }}/replica?lag={{ patroni_maximum_lag_on_replica }}',
          interval: '2s',
        }
      - { args: ['systemctl', 'status', 'pgbouncer'], interval: '5s' }
#  - name: "{{ patroni_cluster_name }}"
#    id: "{{ patroni_cluster_name }}-sync-replica"
#    tags: ['sync-replica']
#    port: "{{ pgbouncer_listen_port }}"
#    checks:
#      - { http: "http://{{ inventory_hostname }}:{{ patroni_restapi_port }}/sync", interval: "2s" }
#      - { args: ["systemctl", "status", "pgbouncer"], interval: "5s" }
#  - name: "{{ patroni_cluster_name }}"
#    id: "{{ patroni_cluster_name }}-async-replica"
#    tags: ['async-replica']
#    port: "{{ pgbouncer_listen_port }}"
#    checks:
#      - { http: "http://{{ inventory_hostname }}:{{ patroni_restapi_port }}/async?lag={{ patroni_maximum_lag_on_replica }}", interval: "2s" }
#      - { args: ["systemctl", "status", "pgbouncer"], interval: "5s" }

# TLS certificate (for PostgreSQL, PgBouncer and etcd)
tls_cert_generate: true
tls_cert_valid_days: 3650
tls_cert_path: '/etc/tls/server.crt'
tls_privatekey_path: '/etc/tls/server.key'
tls_ca_cert_path: '/etc/tls/ca.crt'
tls_owner: 'postgres'
tls_etcd_cert_path: '/etc/etcd/tls/server.crt'
tls_etcd_ca_cert_path: '/etc/etcd/tls/ca.crt'
tls_etcd_privatekey_path: '/etc/etcd/tls/server.key'

# PostgreSQL variables
postgresql_version: 16
# postgresql_data_dir: see vars/Debian.yml or vars/RedHat.yml
postgresql_listen_addr: '0.0.0.0' # Listen on all interfaces. Or use "{{ inventory_hostname }},127.0.0.1" to listen on a specific IP address.
postgresql_port: 5432
postgresql_encoding: 'UTF8' # for bootstrap only (initdb)
postgresql_locale: 'en_US.UTF-8' # for bootstrap only (initdb)
postgresql_data_checksums: true # for bootstrap only (initdb)
postgresql_password_encryption_algorithm: 'scram-sha-256' # or "md5" if your clients do not work with passwords encrypted with SCRAM-SHA-256

# (optional) list of users to be created (if not already exists)
postgresql_users:
  - { name: '{{ pgbouncer_auth_username }}', password: '{{ pgbouncer_auth_password }}', flags: 'LOGIN', role: '' }
  - { name: 'git', password: 'q3CJZI5khqs1GyjIgWYqcexk5yZsfquUQ', flags: 'LOGIN' } # monitoring Service Account
#  - { name: "mydb-user", password: "mydb-user-pass", flags: "SUPERUSER" }
#  - { name: "", password: "", flags: "NOSUPERUSER" }
#  - { name: "", password: "", flags: "NOSUPERUSER" }
#  - { name: "", password: "", flags: "NOLOGIN" }

# (optional) list of databases to be created (if not already exists)
postgresql_databases:
  # []
  - { db: 'pgbench', encoding: 'UTF8', lc_collate: 'en_US.UTF-8', lc_ctype: 'en_US.UTF-8', owner: 'git' }
  - { db: 'git_db', encoding: 'UTF8', lc_collate: 'en_US.UTF-8', lc_ctype: 'en_US.UTF-8', owner: 'git' }
#  - { db: "mydatabase2", encoding: "UTF8", lc_collate: "ru_RU.UTF-8", lc_ctype: "ru_RU.UTF-8", owner: "mydb-user", conn_limit: "50" }
#  - { db: "", encoding: "UTF8", lc_collate: "en_US.UTF-8", lc_ctype: "en_US.UTF-8", owner: "" }
#  - { db: "", encoding: "UTF8", lc_collate: "en_US.UTF-8", lc_ctype: "en_US.UTF-8", owner: "" }

# (optional) list of schemas to be created (if not already exists)
postgresql_schemas: []
#  - { schema: "myschema", db: "mydatabase", owner: "mydb-user" }

# (optional) list of privileges to be granted (if not already exists) or revoked
# https://docs.ansible.com/ansible/latest/collections/community/postgresql/postgresql_privs_module.html#examples
# The db (which is the database to connect to) and role parameters are required
postgresql_privs:
  #  - { role: "test", privs: "SELECT,INSERT,UPDATE", type: "table", db: "test2", objs: "test" }  # grant SELECT, INSERT, UPDATE on a table to role test
  - { role: 'git', privs: 'ALL', type: 'database', db: 'git_db', objs: 'git_db' } # grant ALL on a database to role test-user
#  - { role: "mydb-user", privs: "SELECT", type: "table", db: "mydb", objs: "my_table", schema: "my_schema" }  # grant SELECT on a table and schema
#  - { role: "user", privs: "EXECUTE", type: "function", db: "db1", objs: "pg_ls_waldir()", schema: "pg_catalog" }  # grant EXECUTE on a function
#  - { role: "user, privs: "SELECT", type: "table", db: "mydb", objs: "table2", schema: "schema2", state: "absent" }  # revoke SELECT on a table2 and schema2
#  - { role: "test, test2", privs: "CREATE", type: "database", db: "test2", objs: "test2" }  # grant CREATE on a database test2 to role test and test2

# (optional) list of database extensions to be created (if not already exists)
postgresql_extensions: []
#  - { ext: "pg_stat_statements", db: "postgres" }
#  - { ext: "pg_stat_statements", db: "mydatabase" }
#  - { ext: "pg_stat_statements", db: "mydatabase", schema: "myschema" }
#  - { ext: "pg_stat_statements", db: "" }
#  - { ext: "pg_stat_statements", db: "" }
#  - { ext: "pg_repack", db: "" }  # postgresql-<version>-repack package is required
#  - { ext: "pg_stat_kcache", db: "" }  # postgresql-<version>-pg-stat-kcache package is required
#  - { ext: "", db: "" }
#  - { ext: "", db: "" }

# postgresql parameters to bootstrap dcs (are parameters for example)
postgresql_parameters:
  - { option: 'max_connections', value: '1000' }
  - { option: 'superuser_reserved_connections', value: '5' }
  - { option: 'password_encryption', value: '{{ postgresql_password_encryption_algorithm }}' }
  - { option: 'ssl', value: "{{ 'on' if tls_cert_generate | bool else 'off' }}" }
  - { option: 'ssl_prefer_server_ciphers', value: "{{ 'on' if tls_cert_generate | bool else 'off' }}" }
  - { option: 'ssl_cert_file', value: "{{ tls_cert_path | default('') }}" }
  - { option: 'ssl_key_file', value: "{{ tls_privatekey_path | default('') }}" }
  - { option: 'ssl_ca_file', value: "{{ tls_ca_cert_path | default('') }}" }
  - { option: 'ssl_min_protocol_version', value: 'TLSv1.2' }
  - { option: 'max_locks_per_transaction', value: '512' }
  - { option: 'max_prepared_transactions', value: '0' }
  - { option: 'huge_pages', value: 'try' } # "vm.nr_hugepages" is auto-configured for shared_buffers >= 8GB (if huge_pages_auto_conf is true)
  - { option: 'shared_buffers', value: '{{ (ansible_memtotal_mb * 0.25) | int }}MB' } # by default, 25% of RAM
  - { option: 'effective_cache_size', value: '{{ (ansible_memtotal_mb * 0.75) | int }}MB' } # by default, 75% of RAM
  - { option: 'work_mem', value: '128MB' } # please change this value
  - { option: 'maintenance_work_mem', value: '256MB' } # please change this value
  - { option: 'checkpoint_timeout', value: '15min' }
  - { option: 'checkpoint_completion_target', value: '0.9' }
  - { option: 'min_wal_size', value: '2GB' }
  - { option: 'max_wal_size', value: '8GB' } # or 16GB/32GB
  - { option: 'wal_buffers', value: '32MB' }
  - { option: 'default_statistics_target', value: '1000' }
  - { option: 'seq_page_cost', value: '1' }
  - { option: 'random_page_cost', value: '1.1' } # or "4" for HDDs with slower random access
  - { option: 'effective_io_concurrency', value: '200' } # or "2" for traditional HDDs with lower I/O parallelism
  - { option: 'synchronous_commit', value: 'on' } # or 'off' if you can you lose single transactions in case of a crash
  - { option: 'autovacuum', value: 'on' } # never turn off the autovacuum!
  - { option: 'autovacuum_max_workers', value: '5' }
  - { option: 'autovacuum_vacuum_scale_factor', value: '0.01' } # or 0.005/0.001
  - { option: 'autovacuum_analyze_scale_factor', value: '0.01' }
  - { option: 'autovacuum_vacuum_cost_limit', value: '500' } # or 1000/5000
  - { option: 'autovacuum_vacuum_cost_delay', value: '2' }
  - { option: 'autovacuum_naptime', value: '1s' }
  - { option: 'max_files_per_process', value: '4096' }
  - { option: 'archive_mode', value: 'on' }
  - { option: 'archive_timeout', value: '1800s' }
  - { option: 'archive_command', value: 'cd .' } # not doing anything yet with WAL-s
  #  - { option: "archive_command", value: "{{ wal_g_archive_command }}" }  # archive WAL-s using WAL-G
  #  - { option: "archive_command", value: "{{ pgbackrest_archive_command }}" }  # archive WAL-s using pgbackrest
  - { option: 'wal_level', value: 'logical' }
  - { option: 'wal_keep_size', value: '2GB' }
  - { option: 'max_wal_senders', value: '10' }
  - { option: 'max_replication_slots', value: '10' }
  - { option: 'hot_standby', value: 'on' }
  - { option: 'wal_log_hints', value: 'on' }
  - { option: 'wal_compression', value: 'on' }
  - { option: 'shared_preload_libraries', value: 'pg_stat_statements,auto_explain' }
  - { option: 'pg_stat_statements.max', value: '10000' }
  - { option: 'pg_stat_statements.track', value: 'all' }
  - { option: 'pg_stat_statements.track_utility', value: 'false' }
  - { option: 'pg_stat_statements.save', value: 'true' }
  - { option: 'auto_explain.log_min_duration', value: '10s' } # enable auto_explain for 10-second logging threshold. Decrease this value if necessary
  - { option: 'auto_explain.log_analyze', value: 'true' }
  - { option: 'auto_explain.log_buffers', value: 'true' }
  - { option: 'auto_explain.log_timing', value: 'false' }
  - { option: 'auto_explain.log_triggers', value: 'true' }
  - { option: 'auto_explain.log_verbose', value: 'true' }
  - { option: 'auto_explain.log_nested_statements', value: 'true' }
  - { option: 'auto_explain.sample_rate', value: '0.01' } # enable auto_explain for 1% of queries logging threshold
  - { option: 'track_io_timing', value: 'on' }
  - { option: 'log_lock_waits', value: 'on' }
  - { option: 'log_temp_files', value: '0' }
  - { option: 'track_activities', value: 'on' }
  - { option: 'track_activity_query_size', value: '4096' }
  - { option: 'track_counts', value: 'on' }
  - { option: 'track_functions', value: 'all' }
  - { option: 'log_checkpoints', value: 'on' }
  - { option: 'logging_collector', value: 'on' }
  - { option: 'log_truncate_on_rotation', value: 'on' }
  - { option: 'log_rotation_age', value: '1d' }
  - { option: 'log_rotation_size', value: '0' }
  - { option: 'log_line_prefix', value: '%t [%p-%l] %r %q%u@%d ' }
  - { option: 'log_filename', value: 'postgresql-%a.log' }
  - { option: 'log_directory', value: '{{ postgresql_log_dir }}' }
  - { option: 'hot_standby_feedback', value: 'on' } # allows feedback from a hot standby to the primary that will avoid query conflicts
  - { option: 'max_standby_streaming_delay', value: '30s' }
  - { option: 'wal_receiver_status_interval', value: '10s' }
  - { option: 'idle_in_transaction_session_timeout', value: '10min' } # reduce this timeout if possible
  - { option: 'jit', value: 'off' }
  - { option: 'max_worker_processes', value: '{{ [ansible_processor_vcpus | int, 16] | max }}' }
  - { option: 'max_parallel_workers', value: '{{ [(ansible_processor_vcpus | int // 2), 8] | max }}' }
  - { option: 'max_parallel_workers_per_gather', value: '2' }
  - { option: 'max_parallel_maintenance_workers', value: '2' }
  - { option: 'tcp_keepalives_count', value: '10' }
  - { option: 'tcp_keepalives_idle', value: '300' }
  - { option: 'tcp_keepalives_interval', value: '30' }
#  - { option: "", value: "" }
#  - { option: "", value: "" }

# Set this variable to 'true' if you want the cluster to be automatically restarted
# after changing the 'postgresql_parameters' variable that requires a restart in the 'config_pgcluster.yml' playbook.
# By default, the cluster will not be automatically restarted.
pending_restart: true

# specify additional hosts that will be added to the pg_hba.conf
postgresql_pg_hba:
  - { type: 'local', database: 'all', user: '{{ patroni_superuser_username }}', address: '', method: 'trust' }
  - { type: 'local', database: 'all', user: '{{ pgbouncer_auth_username }}', address: '', method: 'trust' } # required for pgbouncer auth_user
  - { type: 'local', database: 'replication', user: '{{ patroni_replication_username }}', address: '', method: 'trust' }
  - {
      type: 'local',
      database: 'all',
      user: 'all',
      address: '',
      method: '{{ postgresql_password_encryption_algorithm }}',
    }
  - {
      type: 'host',
      database: 'all',
      user: 'all',
      address: '127.0.0.1/32',
      method: '{{ postgresql_password_encryption_algorithm }}',
    }
  - {
      type: 'host',
      database: 'all',
      user: 'all',
      address: '::1/128',
      method: '{{ postgresql_password_encryption_algorithm }}',
    }
  - {
      type: '{{ host_type }}',
      database: 'all',
      user: 'all',
      address: '0.0.0.0/0',
      method: '{{ postgresql_password_encryption_algorithm }}',
    }
#  - { type: "{{ host_type }}", database: "mydatabase", user: "mydb-user", address: "192.168.0.0/24", method: "{{ postgresql_password_encryption_algorithm }}" }
#  - { type: "{{ host_type }}", database: "all", user: "all", address: "192.168.0.0/24", method: "ident", options: "map=main" }  # use pg_ident

host_type: "{{ 'hostssl' if tls_cert_generate | bool else 'host' }}"

# list of lines that Patroni will use to generate pg_ident.conf
postgresql_pg_ident: []
#  - { mapname: "main", system_username: "postgres", pg_username: "backup" }
#  - { mapname: "", system_username: "", pg_username: "" }

# the password file (~/.pgpass)
postgresql_pgpass:
  - 'localhost:{{ postgresql_port }}:*:{{ patroni_superuser_username }}:{{ patroni_superuser_password }}'
  - '{{ inventory_hostname }}:{{ postgresql_port }}:*:{{ patroni_superuser_username }}:{{ patroni_superuser_password }}'
  - '*:{{ pgbouncer_listen_port }}:*:{{ patroni_superuser_username }}:{{ patroni_superuser_password }}'
#  - hostname:port:database:username:password

# PgBouncer parameters
pgbouncer_install: true # or 'false' if you do not want to install and configure the pgbouncer service
pgbouncer_processes: 1 # Number of pgbouncer processes to be used. Multiple processes use the so_reuseport option for better performance.
pgbouncer_conf_dir: '/etc/pgbouncer'
pgbouncer_log_dir: '/var/log/pgbouncer'
pgbouncer_listen_addr: '0.0.0.0' # Listen on all interfaces. Or use "{{ inventory_hostname }}" to listen on a specific IP address.
pgbouncer_listen_port: 6432
pgbouncer_max_client_conn: 100000
pgbouncer_max_db_connections: 10000
pgbouncer_max_prepared_statements: 1024
pgbouncer_query_wait_timeout: 120
pgbouncer_default_pool_size: 100
pgbouncer_default_pool_mode: 'session'
pgbouncer_admin_users: '{{ patroni_superuser_username }}' # comma-separated list of users, who are allowed to change settings
pgbouncer_stats_users: '{{ patroni_superuser_username }}' # comma-separated list of users who are just allowed to use SHOW command
pgbouncer_ignore_startup_parameters: 'extra_float_digits,geqo,search_path'
pgbouncer_auth_type: '{{ postgresql_password_encryption_algorithm }}'
pgbouncer_auth_user: true # or 'false' if you want to manage the list of users for authentication in the database via userlist.txt
pgbouncer_auth_username: pgbouncer # user who can query the database via the user_search function
pgbouncer_auth_password: '' # If not defined, a password will be generated automatically during deployment
pgbouncer_auth_dbname: 'postgres'
pgbouncer_client_tls_sslmode: 'require'
pgbouncer_client_tls_key_file: '{{ tls_privatekey_path }}'
pgbouncer_client_tls_cert_file: '{{ tls_cert_path }}'
pgbouncer_client_tls_ca_file: '{{ tls_ca_cert_path }}'
pgbouncer_client_tls_protocols: 'secure' # allowed values: tlsv1.0, tlsv1.1, tlsv1.2, tlsv1.3, all, secure (tlsv1.2,tlsv1.3)
pgbouncer_client_tls_ciphers: 'secure' # allowed values: default, secure, fast, normal, all (not recommended)
pgbouncer_server_tls_sslmode: 'require'
pgbouncer_server_tls_protocols: 'secure'
pgbouncer_server_tls_ciphers: 'secure'
pgbouncer_server_tls_key_file: '{{ tls_privatekey_path }}'
pgbouncer_server_tls_cert_file: '{{ tls_cert_path }}'
pgbouncer_server_tls_ca_file: '{{ tls_ca_cert_path }}'

pgbouncer_pools:
  - { name: 'postgres', dbname: 'postgres', pool_parameters: '' }
#  - { name: "mydatabase", dbname: "mydatabase", pool_parameters: "pool_size=20 pool_mode=transaction" }
#  - { name: "", dbname: "", pool_parameters: "" }
#  - { name: "", dbname: "", pool_parameters: "" }

# Extended variables (optional)
patroni_restapi_listen_addr: '0.0.0.0' # Listen on all interfaces. Or use "{{ inventory_hostname }}" to listen on a specific IP address.
patroni_restapi_port: 8008
patroni_restapi_username: 'patroni'
patroni_restapi_password: '' # If not defined, a password will be generated automatically during deployment.
patroni_restapi_request_queue_size: 5
patroni_ttl: 30
patroni_loop_wait: 10
patroni_retry_timeout: 10
patroni_master_start_timeout: 300
patroni_maximum_lag_on_failover: 1048576 # (1MB) the maximum bytes a follower may lag to be able to participate in leader election.
patroni_maximum_lag_on_replica: '100MB' # the maximum of lag that replica can be in order to be available for read-only queries.

# https://patroni.readthedocs.io/en/latest/yaml_configuration.html#postgresql
patroni_callbacks: []
#  - {action: "on_role_change", script: ""}
#  - {action: "on_stop", script: ""}
#  - {action: "on_restart", script: ""}
#  - {action: "on_reload", script: ""}
#  - {action: "on_role_change", script: ""}

# https://patroni.readthedocs.io/en/latest/replica_bootstrap.html#standby-cluster
# Requirements:
# 1. the cluster name for Standby Cluster must be unique ('patroni_cluster_name' variable)
# 2. the IP addresses (or network) of the Standby Cluster servers must be added to the pg_hba.conf of the Main Cluster ('postgresql_pg_hba' variable).
patroni_standby_cluster:
  host: '' # an address of remote master
  port: '5432' # a port of remote master
#  primary_slot_name: ""  # which slot on the remote master to use for replication (optional)
#  restore_command: ""  # command to restore WAL records from the remote master to standby leader (optional)
#  recovery_min_apply_delay: ""  # how long to wait before actually apply WAL records on a standby leader (optional)

# Permanent replication slots.
# These slots will be preserved during switchover/failover.
# https://patroni.readthedocs.io/en/latest/dynamic_configuration.html
patroni_slots: []
#  - slot: "logical_replication_slot" # the name of the permanent replication slot.
#    type: "logical" # the type of slot. Could be 'physical' or 'logical' (if the slot is logical, you have to define 'database' and 'plugin').
#    plugin: "pgoutput" # the plugin name for the logical slot.
#    database: "postgres" # the database name where logical slots should be created.
#  - slot: "test_logical_replication_slot"
#    type: "logical"
#    plugin: "pgoutput"
#    database: "test"

patroni_log_destination: stderr # or 'logfile'
# if patroni_log_destination: logfile
patroni_log_dir: /var/log/patroni
patroni_log_level: info
patroni_log_traceback_level: error
patroni_log_format: '%(asctime)s %(levelname)s: %(message)s'
patroni_log_dateformat: ''
patroni_log_max_queue_size: 1000
patroni_log_file_num: 4
patroni_log_file_size: 25000000 # bytes
patroni_log_loggers_patroni_postmaster: warning
patroni_log_loggers_urllib3: warning # or 'debug'

patroni_watchdog_mode: automatic # or 'off', 'required'
patroni_watchdog_device: /dev/watchdog

patroni_postgresql_use_pg_rewind: true # or 'false'
# try to use pg_rewind on the former leader when it joins cluster as a replica.

patroni_remove_data_directory_on_rewind_failure: false # or 'true' (if use_pg_rewind: 'true')
# avoid removing the data directory on an unsuccessful rewind
# if 'true', Patroni will remove the PostgreSQL data directory and recreate the replica.

patroni_remove_data_directory_on_diverged_timelines: false # or 'true'
# if 'true', Patroni will remove the PostgreSQL data directory and recreate the replica
# if it notices that timelines are diverging and the former master can not start streaming from the new master.

# https://patroni.readthedocs.io/en/latest/replica_bootstrap.html#bootstrap
patroni_cluster_bootstrap_method: 'initdb' # or "wal-g", "pgbackrest", "pg_probackup"

# https://patroni.readthedocs.io/en/latest/replica_bootstrap.html#building-replicas
patroni_create_replica_methods:
  #  - pgbackrest
  #  - wal_g
  #  - pg_probackup
  - basebackup

pgbackrest:
  - { option: 'command', value: '{{ pgbackrest_patroni_cluster_restore_command }}' }
  - { option: 'keep_data', value: 'True' }
  - { option: 'no_params', value: 'True' }
wal_g:
  - { option: 'command', value: '{{ wal_g_patroni_cluster_bootstrap_command }}' }
  - { option: 'no_params', value: 'True' }
basebackup:
  - { option: 'max-rate', value: '1000M' }
  - { option: 'checkpoint', value: 'fast' }
#  - { option: "waldir", value: "{{ postgresql_wal_dir }}" }
pg_probackup:
  - { option: 'command', value: '{{ pg_probackup_restore_command }}' }
  - { option: 'no_params', value: 'true' }

# "restore_command" written to recovery.conf when configuring follower (create replica)
postgresql_restore_command: ''
# postgresql_restore_command: "{{ wal_g_path }} wal-fetch %f %p"  # restore WAL-s using WAL-G
# postgresql_restore_command: "pgbackrest --stanza={{ pgbackrest_stanza }} archive-get %f %p"  # restore WAL-s using pgbackrest

# postgresql_restore_command: "pg_probackup-{{ pg_probackup_version }} archive-get -B
# {{ pg_probackup_dir }} --instance {{ pg_probackup_instance }} --wal-file-path=%p
# --wal-file-name=%f"  # restore WAL-s using pg_probackup

# pg_probackup
pg_probackup_install: false # or 'true'
pg_probackup_install_from_postgrespro_repo: true # or 'false'
pg_probackup_version: '{{ postgresql_version }}'
pg_probackup_instance: 'pg_probackup_instance_name'
pg_probackup_dir: '/mnt/backup_dir'
pg_probackup_threads: '4'
pg_probackup_add_keys: '--recovery-target=latest --skip-external-dirs --no-validate'
# ⚠️ Ensure there is a space at the beginning of each part to prevent commands from concatenating.
pg_probackup_command_parts:
  - 'pg_probackup-{{ pg_probackup_version }}'
  - ' restore -B {{ pg_probackup_dir }}'
  - ' --instance {{ pg_probackup_instance }}'
  - ' -j {{ pg_probackup_threads }}'
  - ' {{ pg_probackup_add_keys }}'
pg_probackup_restore_command: "{{ pg_probackup_command_parts | join('') }}"
pg_probackup_patroni_cluster_bootstrap_command: "{{ pg_probackup_command_parts | join('') }}"

# WAL-G
wal_g_install: false # or 'true'
wal_g_version: '3.0.3'
wal_g_installation_method: 'binary' # or "src" to build from source code
wal_g_path: '/usr/local/bin/wal-g --config {{ postgresql_home_dir }}/.walg.json'
wal_g_json: # config https://github.com/wal-g/wal-g#configuration
  - { option: 'AWS_ACCESS_KEY_ID', value: "{{ AWS_ACCESS_KEY_ID | default('') }}" } # define values or pass via --extra-vars
  - { option: 'AWS_SECRET_ACCESS_KEY', value: "{{ AWS_SECRET_ACCESS_KEY | default('') }}" } # define values or pass via --extra-vars
  - { option: 'WALG_S3_PREFIX', value: "{{ WALG_S3_PREFIX | default('s3://' + patroni_cluster_name) }}" } # define values or pass via --extra-vars
  - { option: 'WALG_COMPRESSION_METHOD', value: "{{ WALG_COMPRESSION_METHOD | default('brotli') }}" } # or "lz4", "lzma", "zstd"
  - { option: 'WALG_DELTA_MAX_STEPS', value: "{{ WALG_DELTA_MAX_STEPS | default('6') }}" } # determines how many delta backups can be between full backups
  - { option: 'PGDATA', value: '{{ postgresql_data_dir }}' }
  - { option: 'PGHOST', value: '{{ postgresql_unix_socket_dir }}' }
  - { option: 'PGPORT', value: '{{ postgresql_port }}' }
  - { option: 'PGUSER', value: '{{ patroni_superuser_username }}' }
#  - { option: "AWS_S3_FORCE_PATH_STYLE", value: "true" }  # to use Minio.io S3-compatible storage
#  - { option: "AWS_ENDPOINT", value: "http://minio:9000" }  # to use Minio.io S3-compatible storage
#  - { option: "", value: "" }
wal_g_archive_command: '{{ wal_g_path }} wal-push %p'
wal_g_patroni_cluster_bootstrap_command: '{{ wal_g_path }} backup-fetch {{ postgresql_data_dir }} LATEST'
wal_g_patroni_cluster_bootstrap_recovery_conf:
  - restore_command: '{{ wal_g_path }} wal-fetch %f %p'
  - recovery_target_action: 'promote'
  - recovery_target_timeline: 'latest'
#  - recovery_target_time: "2020-06-01 11:00:00+03"  # Point-in-Time Recovery (example)

# Define job_parts outside of wal_g_cron_jobs
# ⚠️ Ensure there is a space at the beginning of each part to prevent commands from concatenating.
wal_g_backup_command:
  - 'curl -I -s http://{{ inventory_hostname }}:{{ patroni_restapi_port }} | grep 200'
  - ' && {{ wal_g_path }} backup-push {{ postgresql_data_dir }} > {{ postgresql_log_dir }}/walg_backup.log 2>&1'
wal_g_delete_command:
  - 'curl -I -s http://{{ inventory_hostname }}:{{ patroni_restapi_port }} | grep 200'
  - ' && {{ wal_g_path }} delete retain FULL 4 --confirm > {{ postgresql_log_dir }}/walg_delete.log 2>&1'

wal_g_cron_jobs:
  - name: 'WAL-G: Create daily backup'
    user: 'postgres'
    file: /etc/cron.d/walg
    minute: '00'
    hour: "{{ WALG_BACKUP_HOUR | default('3') }}"
    day: '*'
    month: '*'
    weekday: '*'
    job: "{{ wal_g_backup_command | join('') }}"
  - name: 'WAL-G: Delete old backups'
    user: 'postgres'
    file: /etc/cron.d/walg
    minute: '30'
    hour: '6'
    day: '*'
    month: '*'
    weekday: '*'
    job: "{{ wal_g_delete_command | join('') }}"

# pgBackRest
pgbackrest_install: false # or 'true' to install and configure backups using pgBackRest
pgbackrest_install_from_pgdg_repo: true # or 'false'
pgbackrest_stanza: '{{ patroni_cluster_name }}' # specify your --stanza
pgbackrest_repo_type: 'posix' # or "s3", "gcs", "azure"
pgbackrest_repo_host: '' # dedicated repository host (optional)
pgbackrest_repo_user: 'postgres' # if "repo_host" is set (optional)
pgbackrest_conf_file: '/etc/pgbackrest/pgbackrest.conf'
# config https://pgbackrest.org/configuration.html
pgbackrest_conf:
  global: # [global] section
    - { option: 'log-level-file', value: 'detail' }
    - { option: 'log-path', value: '/var/log/pgbackrest' }
    - { option: 'repo1-type', value: '{{ pgbackrest_repo_type | lower }}' }
    #    - { option: "repo1-host", value: "{{ pgbackrest_repo_host }}" }
    #    - { option: "repo1-host-user", value: "{{ pgbackrest_repo_user }}" }
    - { option: 'repo1-path', value: '/var/lib/pgbackrest' }
    - { option: 'repo1-retention-full', value: '4' }
    - { option: 'repo1-retention-archive', value: '4' }
    - { option: 'repo1-bundle', value: 'y' }
    - { option: 'repo1-block', value: 'y' }
    - { option: 'start-fast', value: 'y' }
    - { option: 'stop-auto', value: 'y' }
    - { option: 'link-all', value: 'y' }
    - { option: 'resume', value: 'n' }
    - { option: 'spool-path', value: '/var/spool/pgbackrest' }
    - { option: 'archive-async', value: 'y' } # Enables asynchronous WAL archiving (details: https://pgbackrest.org/user-guide.html#async-archiving)
    - { option: 'archive-get-queue-max', value: '1GiB' }
  #    - { option: "archive-push-queue-max", value: "100GiB" }
  #    - { option: "backup-standby", value: "y" } # When set to 'y', standby servers will be automatically added to the stanza section.
  #    - { option: "", value: "" }
  stanza: # [stanza_name] section
    - { option: 'process-max', value: '4' }
    - { option: 'log-level-console', value: 'info' }
    - { option: 'recovery-option', value: 'recovery_target_action=promote' }
    - { option: 'pg1-socket-path', value: '{{ postgresql_unix_socket_dir }}' }
    - { option: 'pg1-path', value: '{{ postgresql_data_dir }}' }
#    - { option: "", value: "" }
# (optional) dedicated backup server config (if "repo_host" is set)
pgbackrest_server_conf:
  global:
    - { option: 'log-level-file', value: 'detail' }
    - { option: 'log-level-console', value: 'info' }
    - { option: 'log-path', value: '/var/log/pgbackrest' }
    - { option: 'repo1-type', value: '{{ pgbackrest_repo_type | lower }}' }
    - { option: 'repo1-path', value: '/var/lib/pgbackrest' }
    - { option: 'repo1-retention-full', value: '4' }
    - { option: 'repo1-retention-archive', value: '4' }
    - { option: 'repo1-bundle', value: 'y' }
    - { option: 'repo1-block', value: 'y' }
    - { option: 'archive-check', value: 'y' }
    - { option: 'archive-copy', value: 'n' }
    - { option: 'backup-standby', value: 'y' }
    - { option: 'start-fast', value: 'y' }
    - { option: 'stop-auto', value: 'y' }
    - { option: 'link-all', value: 'y' }
    - { option: 'resume', value: 'n' }
#    - { option: "", value: "" }
# the stanza section will be generated automatically

pgbackrest_archive_command: 'pgbackrest --stanza={{ pgbackrest_stanza }} archive-push %p'

pgbackrest_patroni_cluster_restore_command: '/usr/bin/pgbackrest --stanza={{ pgbackrest_stanza }} --delta restore' # restore from latest backup
#  '/usr/bin/pgbackrest --stanza={{ pgbackrest_stanza }} --type=time "--target=2020-06-01 11:00:00+03" --delta restore'  # Point-in-Time Recovery (example)

# By default, the cron jobs is created on the database server.
# If 'repo_host' is defined, the cron jobs will be created on the pgbackrest server.
pgbackrest_cron_jobs:
  - name: 'pgBackRest: Full Backup'
    file: '/etc/cron.d/pgbackrest-{{ patroni_cluster_name }}'
    user: 'postgres'
    minute: '00'
    hour: "{{ PGBACKREST_BACKUP_HOUR | default('3') }}"
    day: '*'
    month: '*'
    weekday: '0'
    job: 'pgbackrest --stanza={{ pgbackrest_stanza }} --type=full backup'
    # job: "if [ $(psql -tAXc 'select pg_is_in_recovery()') = 'f' ]; then pgbackrest --stanza={{ pgbackrest_stanza }} --type=full backup; fi"
  - name: 'pgBackRest: Diff Backup'
    file: '/etc/cron.d/pgbackrest-{{ patroni_cluster_name }}'
    user: 'postgres'
    minute: '00'
    hour: '3'
    day: '*'
    month: '*'
    weekday: '1-6'
    job: 'pgbackrest --stanza={{ pgbackrest_stanza }} --type=diff backup'
    # job: "if [ $(psql -tAXc 'select pg_is_in_recovery()') = 'f' ]; then pgbackrest --stanza={{ pgbackrest_stanza }} --type=diff backup; fi"

# PITR mode (if patroni_cluster_bootstrap_method: "pgbackrest" or "wal-g"):
# 1) The database cluster directory will be cleaned (for "wal-g") or overwritten (for "pgbackrest" --delta restore).
# 2) And also the patroni cluster "{{ patroni_cluster_name }}" will be removed from the DCS (if exist) before recovery.
cluster_restore_timeout: 86400 # backup and WAL restore timeout in seconds (24 hours)

disable_archive_command: true # or 'false' to not disable archive_command after restore
keep_patroni_dynamic_json: true # or 'false' to remove patroni.dynamic.json after restore (if exists)

# Netdata - https://github.com/netdata/netdata
netdata_install: false # or 'true' for install Netdata on postgresql cluster nodes (with kickstart.sh)
netdata_install_options: '--stable-channel --disable-telemetry --dont-wait'
netdata_conf:
  web_bind_to: '*'
  # https://learn.netdata.cloud/docs/store/change-metrics-storage
  memory_mode: 'dbengine' # The long-term metrics storage with efficient RAM and disk usage.
  page_cache_size: 64 # Determines the amount of RAM in MiB that is dedicated to caching Netdata metric values.
  dbengine_disk_space: 1024 # Determines the amount of disk space in MiB that is dedicated to storing Netdata metric values.

### Debian --------------------------

# PostgreSQL variables
postgresql_cluster_name: "main"
# You can specify custom data dir path. Example: "/pgdata/{{ postgresql_version }}/main"
postgresql_data_dir: "\
  {% if cloud_provider | default('') | length > 0 %}\
    {{ pg_data_mount_path | default('/pgdata') }}/{{ postgresql_version }}/{{ postgresql_cluster_name }}\
  {% else %}\
    /var/lib/postgresql/{{ postgresql_version }}/{{ postgresql_cluster_name }}\
  {% endif %}"
# Note: When deploying to cloud providers, we create a disk and mount the data directory,
# along the path defined in the 'pg_data_mount_path' variable (or use '/pgdata' by default).

# You can specify custom WAL dir path. Example: "/pgwal/{{ postgresql_version }}/pg_wal"
postgresql_wal_dir: ""  # if defined, symlink will be created [optional]
postgresql_conf_dir: "/etc/postgresql/{{ postgresql_version }}/{{ postgresql_cluster_name }}"
postgresql_bin_dir: "/usr/lib/postgresql/{{ postgresql_version }}/bin"
postgresql_log_dir: "/var/log/postgresql"
postgresql_unix_socket_dir: "/var/run/postgresql"
postgresql_home_dir: "/var/lib/postgresql"

# stats_temp_directory (mount the statistics directory in tmpfs)
# if postgresql_version < 15
postgresql_stats_temp_directory_path: "/var/lib/pgsql_stats_tmp"  # or 'none'
postgresql_stats_temp_directory_size: "1024m"

# Repository
apt_repository:
  - repo: "deb https://apt.postgresql.org/pub/repos/apt/ {{ ansible_distribution_release }}-pgdg main"  # postgresql apt repository
    key: "https://apt.postgresql.org/pub/repos/apt/ACCC4CF8.asc"  # postgresql apt repository key
#  - repo: "deb https://deb.debian.org/debian/ {{ ansible_distribution_release }} main"
#  - repo: "deb https://deb.debian.org/debian/ {{ ansible_distribution_release }}-updates main"
#  - repo: "deb https://security.debian.org/debian-security/ {{ ansible_distribution_release }}/updates main"

# Packages
system_packages:
  - python3
  - python3-dev
  - python3-psycopg2
  - python3-setuptools
  - python3-pip
  - curl
  - less
  - sudo
  - vim
  - gcc
  - jq
  - iptables
  - acl
  - dnsutils
  - moreutils
  - unzip
  - tar
  - zstd

install_perf: false  # or 'true' to install "perf" (Linux profiling with performance counters) and "FlameGraph".

postgresql_packages:
  - postgresql-{{ postgresql_version }}
  - postgresql-client-{{ postgresql_version }}
  - postgresql-contrib-{{ postgresql_version }}
  - postgresql-server-dev-{{ postgresql_version }}
  - postgresql-{{ postgresql_version }}-dbgsym
#  - postgresql-{{ postgresql_version }}-repack
#  - postgresql-{{ postgresql_version }}-cron
#  - postgresql-{{ postgresql_version }}-pg-stat-kcache
#  - postgresql-{{ postgresql_version }}-pg-wait-sampling
#  - postgresql-{{ postgresql_version }}-postgis-3
#  - postgresql-{{ postgresql_version }}-pgrouting
#  - postgresql-{{ postgresql_version }}-pgvector
#  - postgresql-{{ postgresql_version }}-pgaudit
#  - postgresql-{{ postgresql_version }}-partman

# Extra packages
etcd_package_repo: "https://github.com/etcd-io/etcd/releases/download/v{{ etcd_version }}/etcd-v{{ etcd_version }}-linux-amd64.tar.gz"
vip_manager_package_repo:
  "https://github.com/cybertec-postgresql/vip-manager/releases/download/v{{ vip_manager_version }}/vip-manager_{{ vip_manager_version }}_Linux_x86_64.deb"

installation_method: "repo"  # "repo" (default) or "file"

# The Patroni package will be installed from the deb package by default.
# You also have the option of choosing an installation method using the pip package.
patroni_installation_method: "deb"  # "deb" (default) or "pip"

# if patroni_installation_method: "pip"
patroni_install_version: "latest"  # or a specific version (e.q., '3.3.2')

# if patroni_installation_method: "deb"
patroni_packages:
  - patroni
  - python3-{{ dcs_type }}

# if patroni_installation_method: "deb" (optional)
# You can preload the patroni deb package to your APT repository, or explicitly specify the path to the package in this variable:
patroni_deb_package_repo: []
#  - "https://apt.postgresql.org/pub/repos/apt/pool/main/p/patroni/patroni_3.3.0-1.pgdg22.04%2B1_all.deb"  # (package for Ubuntu 22.04)

# if patroni_installation_method: "pip" (optional)
# Packages from your repository will be used to install. By default, it is installed from the public pip repository.
pip_package_repo: "https://bootstrap.pypa.io/get-pip.py"  # latest version pip3 for python3 (or use "pip-<version>.tar.gz").
patroni_pip_requirements_repo: []
#  - "http://my-repo.url/setuptools-41.2.0.zip"
#  - "http://my-repo.url/setuptools_scm-3.3.3.tar.gz"
#  - "http://my-repo.url/urllib3-1.24.3.tar.gz"
#  - "http://my-repo.url/PyYAML-5.1.2.tar.gz"
#  - "http://my-repo.url/chardet-3.0.4.tar.gz"
#  - "http://my-repo.url/idna-2.8.tar.gz"
#  - "http://my-repo.url/certifi-2019.9.11.tar.gz"
#  - "http://my-repo.url/requests-2.22.0.tar.gz"
#  - "http://my-repo.url/six-1.12.0.tar.gz"
#  - "http://my-repo.url/kazoo-2.6.1.tar.gz"
#  - "http://my-repo.url/dnspython-1.16.0.zip"
#  - "http://my-repo.url/python-etcd-0.4.5.tar.gz"
#  - "http://my-repo.url/Click-7.0.tar.gz"
#  - "http://my-repo.url/prettytable-0.7.2.tar.gz"
#  - "http://my-repo.url/pytz-2019.2.tar.gz"
#  - "http://my-repo.url/tzlocal-2.0.0.tar.gz"
#  - "http://my-repo.url/wheel-0.33.6.tar.gz"
#  - "http://my-repo.url/python-dateutil-2.8.0.tar.gz"
#  - "http://my-repo.url/psutil-5.6.3.tar.gz"
#  - "http://my-repo.url/cdiff-1.0.tar.gz"
patroni_pip_package_repo: []
# - "http://my-repo.url/patroni-1.6.0.tar.gz"

# if with_haproxy_load_balancing: true
haproxy_installation_method: "deb"  # "deb" (default) or "src"
confd_package_repo: "https://github.com/kelseyhightower/confd/releases/download/v0.16.0/confd-0.16.0-linux-amd64"

# if haproxy_installation_method: 'src' (optional)
haproxy_major: "1.8"
haproxy_version: "1.8.31"  # version to build from source
lua_src_repo: "https://www.lua.org/ftp/lua-5.3.5.tar.gz"  # required for build haproxy
haproxy_src_repo: "https://www.haproxy.org/download/{{ haproxy_major }}/src/haproxy-{{ haproxy_version }}.tar.gz"
haproxy_compile_requirements:
  - unzip
  - gzip
  - make
  - gcc
  - build-essential
  - libc6-dev
  - libpcre3-dev
  - liblua5.3-dev
  - libreadline-dev
  - zlib1g-dev
  - libsystemd-dev
  - ca-certificates
  - libssl-dev


# ================================================================================================= #
# Offline installation (if installation_method: "file")
#
# You can also download the necessary packages into /autobase/automation/files/ directory.
# Packages from this directory will be used for installation.

# if installation_method: "file" and patroni_installation_method: "deb"
patroni_deb_package_file: "patroni_3.3.0-1.pgdg22.04%2B1_all.deb"
# (package for Ubuntu 22.04) https://apt.postgresql.org/pub/repos/apt/pool/main/p/patroni/

# if installation_method: "file" and patroni_installation_method: "pip"
pip_package_file: "pip-24.2.tar.gz"  # https://pypi.org/project/pip/#files
patroni_pip_requirements_file: []
#  - "setuptools-41.2.0.zip"  # https://pypi.org/project/setuptools/#files
#  - "setuptools_scm-3.3.3.tar.gz"  # https://pypi.org/project/setuptools-scm/#files
#  - "urllib3-1.24.3.tar.gz"  # https://pypi.org/project/urllib3/1.24.3/#files
#  - "PyYAML-5.1.2.tar.gz"  # https://pypi.org/project/PyYAML/#files
#  - "chardet-3.0.4.tar.gz"  # https://pypi.org/project/chardet/#files # (required for "requests")
#  - "idna-2.8.tar.gz"  # https://pypi.org/project/idna/#files    # (required for "requests")
#  - "certifi-2019.9.11.tar.gz"  # https://pypi.org/project/certifi/#files # (required for "requests")
#  - "requests-2.22.0.tar.gz"  # https://pypi.org/project/requests/#files
#  - "six-1.12.0.tar.gz"  # https://pypi.org/project/six/#files
#  - "kazoo-2.6.1.tar.gz"  # https://pypi.org/project/kazoo/#files
#  - "dnspython-1.16.0.zip"  # https://pypi.org/project/dnspython/#files # (required for "python-etcd")
#  - "python-etcd-0.4.5.tar.gz"  # https://pypi.org/project/python-etcd/#files
#  - "Click-7.0.tar.gz"  # https://pypi.org/project/click/#files
#  - "prettytable-0.7.2.tar.gz"  # https://pypi.org/project/PrettyTable/#files
#  - "pytz-2019.2.tar.gz"  # https://pypi.org/project/pytz/#files # (required for "tzlocal")
#  - "tzlocal-2.0.0.tar.gz"  # https://pypi.org/project/tzlocal/#files
#  - "wheel-0.33.6.tar.gz"  # https://pypi.org/project/wheel/#files
#  - "python-dateutil-2.8.0.tar.gz"  # https://pypi.org/project/python-dateutil/#files
#  - "psutil-5.6.3.tar.gz"  # https://pypi.org/project/psutil/#files
#  - "cdiff-1.0.tar.gz"  # https://pypi.org/project/cdiff/#files
patroni_pip_package_file: []
#  - "patroni-3.3.2.tar.gz"  # https://pypi.org/project/patroni/#files

# additional packages
etcd_package_file: "etcd-v3.5.15-linux-amd64.tar.gz"  # https://github.com/etcd-io/etcd/releases
vip_manager_package_file: "vip-manager_2.6.0_Linux_x86_64.deb"  # https://github.com/cybertec-postgresql/vip-manager/releases
wal_g_package_file: "wal-g-pg-ubuntu20.04-aarch64.tar.gz"  # https://github.com/wal-g/wal-g/releases

# if with_haproxy_load_balancing: true
haproxy_package_file: []
#  - "haproxy_1.8.25-1~bpo9+1_amd64.deb"
confd_package_file: "confd-0.16.0-linux-amd64"  # https://github.com/kelseyhightower/confd/releases

# if haproxy_installation_method: 'src' (optional)
lua_src_file: "lua-5.3.5.tar.gz"  # https://www.lua.org/ftp/lua-5.3.5.tar.gz (required for build haproxy)
haproxy_src_file: "haproxy-1.8.31.tar.gz"  # http://www.haproxy.org/download/1.8/src/

# ------------------------------------------------------------------------------------------------- #
# (optional) Specify additional deb packages if required (for any installation_method)
# this packages will be installed before all other packages.
packages_from_file: []
#  - "my-package-name_1_amd64.deb"
#  - "my-package-name_2_amd64.deb"
#  - ""

# ----------------------------------------------------------------------------------------------------------------------------------
# Attention! If you want to use the installation method "file".
# You need to independently determine all the necessary the dependencies of deb packages for your version of the Linux distribution.
# ----------------------------------------------------------------------------------------------------------------------------------

### RedHat --------------------------

# PostgreSQL variables
#
# You can specify custom data dir path. Example: "/pgdata/{{ postgresql_version }}/data"
postgresql_data_dir: "\
  {% if cloud_provider | default('') | length > 0 %}\
  {{ pg_data_mount_path | default('/pgdata') }}/{{ postgresql_version }}/data\
  {% else %}\
  /var/lib/pgsql/{{ postgresql_version }}/data\
  {% endif %}"
# Note: When deploying to cloud providers, we create a disk and mount the data directory,
# along the path defined in the 'pg_data_mount_path' variable (or use '/pgdata' by default).

# You can specify custom WAL dir path. Example: "/pgwal/{{ postgresql_version }}/pg_wal"
postgresql_wal_dir: '' # if defined, symlink will be created [optional]
postgresql_conf_dir: '{{ postgresql_data_dir }}'
postgresql_bin_dir: '/usr/pgsql-{{ postgresql_version }}/bin'
postgresql_log_dir: '/var/log/postgresql'
postgresql_unix_socket_dir: '/var/run/postgresql'
postgresql_home_dir: '/var/lib/pgsql'

# stats_temp_directory (mount the statistics directory in tmpfs)
# if postgresql_version < 15
postgresql_stats_temp_directory_path: '/var/lib/pgsql_stats_tmp' # or 'none'
postgresql_stats_temp_directory_size: '1024m'

# Repository
yum_repository: []
#  - name: "repo name"
#    description: "repo description"
#    baseurl: "https://my-repo.url"
#    gpgkey: "https://my-repo-key.url"
#    gpgcheck: "yes"

install_postgresql_repo: true # or 'false' (installed from the package "pgdg-redhat-repo-latest.noarch.rpm")
install_epel_repo: true # or 'false' (installed from the package "epel-release-latest.noarch.rpm")
install_scl_repo: true # or 'false' (Redhat 7 family only)

# Packages (for yum repo)
python_version: '3' # override the version (e.q, 3.11) only if patroni_installation_method: "pip" is used

os_specific_packages:
  RedHat-8:
    - python2
    - python3-libselinux
    - python3-libsemanage
    - python3-policycoreutils
    - dnf-utils
  RedHat-9:
    - python3-libselinux
    - python3-libsemanage
    - python3-policycoreutils
    - dnf-utils
system_packages:
  - "{{ os_specific_packages[ansible_os_family ~ '-' ~ ansible_distribution_major_version] }}"
  - python{{ python_version }}
  - python{{ python_version }}-devel
  - python{{ python_version }}-psycopg2
  - python{{ python_version }}-setuptools
  - python{{ python_version }}-pip
  - python{{ python_version }}-urllib3
  - less
  - sudo
  - vim
  - gcc
  - jq
  - iptables
  - acl
  - bind-utils
  - moreutils
  - unzip
  - tar
  - zstd

install_perf: false # or 'true' to install "perf" (Linux profiling with performance counters) and "FlameGraph".

# The glibc-langpack package includes the basic information required to support the language in your applications.
# for RHEL version 8 (only)
glibc_langpack:
  - 'glibc-langpack-en'
#  - "glibc-langpack-ru"
#  - "glibc-langpack-de"

postgresql_packages:
  - postgresql{{ postgresql_version }}
  - postgresql{{ postgresql_version }}-server
  - postgresql{{ postgresql_version }}-contrib
  - postgresql{{ postgresql_version }}-devel
  - postgresql{{ postgresql_version }}-debuginfo
#  - pg_repack_{{ postgresql_version }}
#  - pg_cron_{{ postgresql_version }}
#  - pg_stat_kcache_{{ postgresql_version }}
#  - pg_wait_sampling_{{ postgresql_version }}
#  - postgis33_{{ postgresql_version }}
#  - pgrouting_{{ postgresql_version }}
#  - pgvector_{{ postgresql_version }}
#  - pgaudit17_{{ postgresql_version }}
#  - pg_partman_{{ postgresql_version }}

# Extra packages
etcd_package_repo: 'https://github.com/etcd-io/etcd/releases/download/v{{ etcd_version }}/etcd-v{{ etcd_version }}-linux-amd64.tar.gz'
vip_manager_package_repo: 'https://github.com/cybertec-postgresql/vip-manager/releases/download/v{{ vip_manager_version }}/vip-manager_{{ vip_manager_version }}_Linux_x86_64.rpm'

installation_method: 'repo' # "repo" (default) or "file"

# The Patroni package will be installed from the rpm package by default.
# You also have the option of choosing an installation method using the pip package.
patroni_installation_method: 'rpm' # "rpm" (default) or "pip"

# if patroni_installation_method: "pip"
patroni_install_version: 'latest' # or a specific version (e.q., '3.3.2')

# if patroni_installation_method: "rpm"
patroni_packages:
  - patroni
  - patroni-{{ dcs_type }}

# if patroni_installation_method: "rpm" (optional)
# You can preload the patroni rpm package to your YUM repository, or explicitly specify the path to the package in this variable:
patroni_rpm_package_repo: []
#  - "https://download.postgresql.org/pub/repos/yum/common/redhat/rhel-8-x86_64/patroni-3.3.2-1PGDG.rhel8.noarch.rpm"  # (package for RHEL 8)

# if patroni_installation_method: "pip" (optional)
# Packages from your repository will be used to install. By default, it is installed from the public pip repository.
pip_package_repo: 'https://bootstrap.pypa.io/get-pip.py' # latest version pip3 for python3 (or use "pip-<version>.tar.gz").
patroni_pip_requirements_repo: []
#  - "http://my-repo.url/setuptools-41.2.0.zip"
#  - "http://my-repo.url/setuptools_scm-3.3.3.tar.gz"
#  - "http://my-repo.url/urllib3-1.24.3.tar.gz"
#  - "http://my-repo.url/PyYAML-5.1.2.tar.gz"
#  - "http://my-repo.url/chardet-3.0.4.tar.gz"
#  - "http://my-repo.url/idna-2.8.tar.gz"
#  - "http://my-repo.url/certifi-2019.9.11.tar.gz"
#  - "http://my-repo.url/requests-2.22.0.tar.gz"
#  - "http://my-repo.url/six-1.12.0.tar.gz"
#  - "http://my-repo.url/kazoo-2.6.1.tar.gz"
#  - "http://my-repo.url/dnspython-1.16.0.zip"
#  - "http://my-repo.url/python-etcd-0.4.5.tar.gz"
#  - "http://my-repo.url/Click-7.0.tar.gz"
#  - "http://my-repo.url/prettytable-0.7.2.tar.gz"
#  - "http://my-repo.url/pytz-2019.2.tar.gz"
#  - "http://my-repo.url/tzlocal-2.0.0.tar.gz"
#  - "http://my-repo.url/wheel-0.33.6.tar.gz"
#  - "http://my-repo.url/python-dateutil-2.8.0.tar.gz"
#  - "http://my-repo.url/psutil-5.6.3.tar.gz"
#  - "http://my-repo.url/cdiff-1.0.tar.gz"
patroni_pip_package_repo: []
#  - "http://my-repo.url/patroni-1.6.0.tar.gz"

# if with_haproxy_load_balancing: true
haproxy_installation_method: 'rpm' # "rpm" (default) or "src"
confd_package_repo: 'https://github.com/kelseyhightower/confd/releases/download/v0.16.0/confd-0.16.0-linux-amd64'

# if haproxy_installation_method: 'src' (optional)
haproxy_major: '1.8'
haproxy_version: '1.8.31' # version to build from source
lua_src_repo: 'https://www.lua.org/ftp/lua-5.3.5.tar.gz' # required for build haproxy
haproxy_src_repo: 'https://www.haproxy.org/download/{{ haproxy_major }}/src/haproxy-{{ haproxy_version }}.tar.gz'
haproxy_compile_requirements:
  - unzip
  - gzip
  - make
  - gcc
  - gcc-c++
  - pcre-devel
  - zlib-devel
  - readline-devel
  - openssl
  - openssl-devel
  - openssl-libs
  - systemd-devel

# ================================================================================================= #
# Offline installation (if installation_method: "file")
#
# You can also download the necessary packages into /autobase/automation/files/ directory.
# Packages from this directory will be used for installation.

# if installation_method: "file" and patroni_installation_method: "rpm"
patroni_rpm_package_file: 'patroni-3.3.2-1PGDG.rhel8.noarch.rpm'
# (package for RHEL 8) https://download.postgresql.org/pub/repos/yum/common/redhat/rhel-8-x86_64/

# if installation_method: "file" and patroni_installation_method: "pip"
pip_package_file: 'pip-24.2.tar.gz' # https://pypi.org/project/pip/#files
patroni_pip_requirements_file: []
#  - "setuptools-41.2.0.zip"  # https://pypi.org/project/setuptools/#files
#  - "setuptools_scm-3.3.3.tar.gz"  # https://pypi.org/project/setuptools-scm/#files
#  - "urllib3-1.24.3.tar.gz"  # https://pypi.org/project/urllib3/1.24.3/#files
#  - "PyYAML-5.1.2.tar.gz"  # https://pypi.org/project/PyYAML/#files
#  - "chardet-3.0.4.tar.gz"  # https://pypi.org/project/chardet/#files # (required for "requests")
#  - "idna-2.8.tar.gz"  # https://pypi.org/project/idna/#files    # (required for "requests")
#  - "certifi-2019.9.11.tar.gz"  # https://pypi.org/project/certifi/#files # (required for "requests")
#  - "requests-2.22.0.tar.gz"  # https://pypi.org/project/requests/#files
#  - "six-1.12.0.tar.gz"  # https://pypi.org/project/six/#files
#  - "kazoo-2.6.1.tar.gz"  # https://pypi.org/project/kazoo/#files
#  - "dnspython-1.16.0.zip"  # https://pypi.org/project/dnspython/#files # (required for "python-etcd")
#  - "python-etcd-0.4.5.tar.gz"  # https://pypi.org/project/python-etcd/#files
#  - "Click-7.0.tar.gz"  # https://pypi.org/project/click/#files
#  - "prettytable-0.7.2.tar.gz"  # https://pypi.org/project/PrettyTable/#files
#  - "pytz-2019.2.tar.gz"  # https://pypi.org/project/pytz/#files # (required for "tzlocal")
#  - "tzlocal-2.0.0.tar.gz"  # https://pypi.org/project/tzlocal/#files
#  - "wheel-0.33.6.tar.gz"  # https://pypi.org/project/wheel/#files
#  - "python-dateutil-2.8.0.tar.gz"  # https://pypi.org/project/python-dateutil/#files
#  - "psutil-5.6.3.tar.gz"  # https://pypi.org/project/psutil/#files
#  - "cdiff-1.0.tar.gz"  # https://pypi.org/project/cdiff/#files
patroni_pip_package_file: []
#  - "patroni-3.3.2.tar.gz"  # https://pypi.org/project/patroni/#files

# additional packages
etcd_package_file: 'etcd-v3.5.15-linux-amd64.tar.gz' # https://github.com/etcd-io/etcd/releases
vip_manager_package_file: 'vip-manager-1.0.2-1.x86_64.rpm' # https://github.com/cybertec-postgresql/vip-manager/releases
wal_g_package_file: 'wal-g.linux-amd64.tar.gz' # https://github.com/wal-g/wal-g/releases

# if with_haproxy_load_balancing: true
haproxy_package_file: []
#  - "rh-haproxy18-runtime-3.1-2.el7.x86_64.rpm"
#  - "rh-haproxy18-haproxy-1.8.31-1.el7.x86_64.rpm"
confd_package_file: 'confd-0.16.0-linux-amd64' # https://github.com/kelseyhightower/confd/releases

# if haproxy_installation_method: 'src' (optional)
lua_src_file: 'lua-5.3.5.tar.gz' # https://www.lua.org/ftp/lua-5.3.5.tar.gz (required for build haproxy)
haproxy_src_file: 'haproxy-1.8.31.tar.gz' # http://www.haproxy.org/download/1.8/src/

# ------------------------------------------------------------------------------------------------- #
# (optional) Specify additional rpm packages if required (for any installation_method)
# this packages will be installed before all other packages.
packages_from_file: []
#  - "python3-psycopg2-2.7.7-2.el7.x86_64.rpm"  # https://mirror.linux-ia64.org/epel/7/x86_64/Packages/p/  # (required for patroni rpm)
#  - "libyaml-0.1.4-11.el7_0.x86_64.rpm"  # (required for patroni rpm)
#  - "jq-1.5-1.el7.x86_64.rpm"  # https://mirror.linux-ia64.org/epel/7/x86_64/Packages/j/
#  - "other-package-name_1_amd64.rpm"
#  - ""

# ----------------------------------------------------------------------------------------------------------------------------------
# Attention! If you want to use the installation method "file".
# You need to independently determine all the necessary the dependencies of rpm packages for your version of the Linux distribution.
# ----------------------------------------------------------------------------------------------------------------------------------

### System

# DNS servers (/etc/resolv.conf)
nameservers: []
#  - "8.8.8.8"  # example (Google Public DNS)
#  - "9.9.9.9"  # (Quad9 Public DNS)

# /etc/hosts (optional)
etc_hosts: []
#  - "10.128.64.143 pgbackrest.minio.local minio.local s3.eu-west-3.amazonaws.com"  # example (MinIO)
#  - ""

ntp_enabled: false # or 'true' if you want to install and configure the ntp service
ntp_servers: []
#  - "10.128.64.44"
#  - "10.128.64.45"

timezone: ''
# timezone: "Etc/UTC"
# timezone: "America/New_York"
# timezone: "Europe/Moscow"
# timezone: "Europe/Berlin"

# Generate locale
# (except RHEL>=8,use glibc-langpack)
locale_gen:
  - { language_country: 'en_US', encoding: 'UTF-8' }
#  - { language_country: "ru_RU", encoding: "UTF-8" }
#  - { language_country: "de_DE", encoding: "UTF-8" }
#  - { language_country: "", encoding: "" }

# Set system locale (LANG,LC_ALL)
locale: 'en_US.utf-8'

# Configure swap space (if not already exists)
swap_file_create: true # or 'false'
swap_file_path: /swapfile
swap_file_size_mb: '4096' # change this value for your system

# Kernel parameters
sysctl_set: true # or 'false'
# these parameters for example! Specify kernel options for your system
sysctl_conf:
  etcd_cluster: []
  consul_instances: []
  master: []
  replica: []
  pgbackrest: []
  postgres_cluster:
    - { name: 'vm.overcommit_memory', value: '2' }
    - { name: 'vm.swappiness', value: '1' }
    - { name: 'vm.min_free_kbytes', value: '102400' }
    - { name: 'vm.dirty_expire_centisecs', value: '1000' }
    - { name: 'vm.dirty_background_bytes', value: '67108864' }
    - { name: 'vm.dirty_bytes', value: '536870912' }
    #    - { name: "vm.nr_hugepages", value: "9510" }  # example "9510"=18GB
    - { name: 'vm.zone_reclaim_mode', value: '0' }
    - { name: 'kernel.numa_balancing', value: '0' }
    - { name: 'kernel.sched_autogroup_enabled', value: '0' }
    - { name: 'net.ipv4.ip_nonlocal_bind', value: '1' }
    - { name: 'net.ipv4.ip_forward', value: '1' }
    - { name: 'net.ipv4.ip_local_port_range', value: '10000 65535' }
    - { name: 'net.core.netdev_max_backlog', value: '10000' }
    - { name: 'net.ipv4.tcp_max_syn_backlog', value: '8192' }
    - { name: 'net.core.somaxconn', value: '65535' }
    - { name: 'net.ipv4.tcp_tw_reuse', value: '1' }
  #    - { name: "net.netfilter.nf_conntrack_max", value: "1048576" }
  #    - { name: "kernel.sched_migration_cost_ns", value: "5000000" }
  #    - { name: "", value: "" }
  balancers:
    - { name: 'net.ipv4.ip_nonlocal_bind', value: '1' }
    - { name: 'net.ipv4.ip_forward', value: '1' }
    - { name: 'net.ipv4.ip_local_port_range', value: '10000 65535' }
    - { name: 'net.core.netdev_max_backlog', value: '10000' }
    - { name: 'net.ipv4.tcp_max_syn_backlog', value: '8192' }
    - { name: 'net.core.somaxconn', value: '65535' }
    - { name: 'net.ipv4.tcp_tw_reuse', value: '1' }
#   - { name: "net.netfilter.nf_conntrack_max", value: "1048576" }
#   - { name: "", value: "" }

# Huge Pages
# this setting will automatically configure "vm.nr_hugepages" for shared_buffers of 8GB or more
# if 'sysctl_set' is 'true', "vm.nr_hugepages" is undefined or insufficient in sysctl_conf,
# and "huge_pages" is not 'off' in postgresql_parameters.
huge_pages_auto_conf: true

# Transparent Huge Pages
disable_thp: true # or 'false'

# Max open file limit
set_limits: true # or 'false'
limits_user: 'postgres'
soft_nofile: 65536
hard_nofile: 200000

# I/O Scheduler (optional)
set_scheduler: false # or 'true'
scheduler:
  - { sched: 'deadline', nr_requests: '1024', device: 'sda' }
#  - { sched: "noop" , nr_requests: "1024", device: "sdb" }
#  - { sched: "" , nr_requests: "1024", device: "" }

# Non-multiqueue I/O schedulers:
# cfq         - for desktop systems and slow SATA drives
# deadline    - for SAS drives (recommended for databases)
# noop        - for SSD drives
# Multiqueue I/O schedulers (blk-mq):
# mq-deadline - (recommended for databases)
# none        - (ideal for fast random I/O devices such as NVMe)
# bfq         - (avoid for databases)
# kyber

# SSH Keys (optional)
enable_ssh_key_based_authentication: false # or 'true' for configure SSH Key-Based Authentication
ssh_key_user: 'postgres'
ssh_key_state: 'present'
ssh_known_hosts: "{{ groups['postgres_cluster'] }}"

# List of public SSH keys. These keys will be added to the database server's  ~/.ssh/authorized_keys  file.
ssh_public_keys: []

# sudo
sudo_users:
  - name: 'postgres'
    nopasswd: 'yes' # or "no" to require a password
    commands: 'ALL'
#  - name: "joe" # other user (example)
#    nopasswd: "no"
#    commands: "/usr/bin/find, /usr/bin/less, /usr/bin/tail, /bin/kill"

# Firewall
firewall_enabled_at_boot: false # or 'true' for configure firewall (iptables)

firewall_allowed_tcp_ports_for:
  master: []
  replica: []
  pgbackrest: []
  postgres_cluster:
    - '{{ ansible_ssh_port | default(22) }}'
    - '{{ postgresql_port }}'
    - '{{ pgbouncer_listen_port }}'
    - '{{ patroni_restapi_port }}'
    - '19999' # Netdata
  #    - "10050"  # Zabbix agent
  #    - ""
  etcd_cluster:
    - '{{ ansible_ssh_port | default(22) }}'
    - '2379' # ETCD port
    - '2380' # ETCD port
  #    - ""
  consul_instances:
    - 8300
    - 8301
    - 8302
    - 8500
    - 8600
  balancers:
    - '{{ ansible_ssh_port | default(22) }}'
    - '{{ haproxy_listen_port.master }}' # HAProxy (read/write) master
    - '{{ haproxy_listen_port.replicas }}' # HAProxy (read only) all replicas
    - '{{ haproxy_listen_port.replicas_sync }}' # HAProxy (read only) synchronous replica only
    - '{{ haproxy_listen_port.replicas_async }}' # HAProxy (read only) asynchronous replicas only
    - '{{ haproxy_listen_port.stats }}' # HAProxy stats
#    - ""

firewall_additional_rules_for:
  master: []
  replica: []
  postgres_cluster: []
  pgbackrest: []
  etcd_cluster: []
  consul_instances: []
  balancers:
    - 'iptables -p vrrp -A INPUT -j ACCEPT' # Keepalived (vrrp)
    - 'iptables -p vrrp -A OUTPUT -j ACCEPT' # Keepalived (vrrp)

# disable firewalld (installed by default on RHEL/CentOS) or ufw (installed by default on Ubuntu)
firewall_disable_firewalld: true
firewall_disable_ufw: true

# (optional) - Fetch files from the server in the "master" group. These files can later be copied to all servers.
fetch_files_from_master: []
#  - { src: "/etc/ssl/certs/ssl-cert-snakeoil.pem", dest: "files/ssl-cert-snakeoil.pem" }
#  - { src: "/etc/ssl/private/ssl-cert-snakeoil.key", dest: "files/ssl-cert-snakeoil.key" }
#  - { src: "/path/to/myfile", dest: "files/myfile" }

# (optional) - Copy this files to all servers in the cluster ("master" and "replica" groups)
copy_files_to_all_server: []
#  - { src: "files/ssl-cert-snakeoil.pem", dest: "/etc/ssl/certs/ssl-cert-snakeoil.pem", owner: "postgres", group: "postgres", mode: "0644" }
#  - { src: "files/ssl-cert-snakeoil.key", dest: "/etc/ssl/private/ssl-cert-snakeoil.key", owner: "postgres", group: "postgres", mode: "0600" }
#  - { src: "files/myfile", dest: "/path/to/myfile", owner: "postgres", group: "postgres", mode: "0640" }

# System cron jobs
cron_jobs: []
#  - name: "Example Job one"
#    user: "postgres"
#    file: /etc/cron.d/example_job_one
#    minute: "00"
#    hour: "1"
#    day: "*"
#    month: "*"
#    weekday: "*"
#    job: "echo 'example job one command'"
#  - name: "Example Job two"
#    user: "postgres"
#    file: /etc/cron.d/example_job_two
#    minute: "00"
#    hour: "2"
#    day: "*"
#    month: "*"
#    weekday: "*"
#    job: "echo 'example job two command'"

# (optional) Configure mount points in /etc/fstab and mount the file system (if 'mount.src' is defined)
mount:
  - path: '/pgdata'
    src: '' # device UUID or path.
    fstype: ext4 # if 'zfs' is specified a ZFS pool will be created
    opts: defaults,noatime # not applicable to 'zfs'
    state: mounted
#  - path: "/pgwal"
#    src: ""
#    fstype: ext4
#    opts: defaults,noatime
#    state: mounted

# (optional) Execute custom commands or scripts
# This can be a direct command, a bash script content, or a path to a script on the host
pre_deploy_command: '' # Command or script to be executed before the Postgres cluster deployment
pre_deploy_command_timeout: 3600 # Timeout in seconds
pre_deploy_command_hosts: 'postgres_cluster' # host groups where the pre_deploy_command should be executed
pre_deploy_command_print: true # Print the command in the ansible log
pre_deploy_command_print_result: true # Print the result of the command execution to the ansible log
pre_deploy_command_log: '/var/tmp/pre_deploy_command.log'

post_deploy_command: '' # Command or script to be executed after the Postgres cluster deployment
post_deploy_command_timeout: 3600 # Timeout in seconds
post_deploy_command_hosts: 'postgres_cluster' # host groups where the post_deploy_command should be executed
post_deploy_command_print: true # Print the command in the ansible log
post_deploy_command_print_result: true # Print the result of the command execution to the ansible log
post_deploy_command_log: '/var/tmp/post_deploy_command.log'

### Upgrade

# yamllint disable rule:line-length
# Variables for the pg_upgrade.yml playbook

# Note:
# There is no need to plan additional disk space, because when updating PostgreSQL, hard links are used instead of copying files.
# However, it is required that the pg_old_datadir and pg_new_datadir are located within the same top-level directory (pg_upper_datadir).
# https://www.postgresql.org/docs/current/pgupgrade.html

# PostgreSQL versions
pg_old_version: '' # specify the current (old) version of PostgreSQL
pg_new_version: '' # specify the target version of PostgreSQL for the upgrade

# Paths for old and new PostgreSQL versions
# Adjust these variables if the paths are different from the default value.

# Directory containing binaries for the old PostgreSQL version.
pg_old_bindir: "{{ postgresql_bin_dir | regex_replace('(/$)', '') | replace(postgresql_version | string, pg_old_version | string) }}"
# Data directory path for the old PostgreSQL version.
pg_old_datadir: "{{ postgresql_data_dir | regex_replace('(/$)', '') | replace(postgresql_version | string, pg_old_version | string) }}"
# Configuration directory path for the old PostgreSQL version.
pg_old_confdir: "{{ postgresql_conf_dir | regex_replace('(/$)', '') | replace(postgresql_version | string, pg_old_version | string) }}"

# Directory containing binaries for the new PostgreSQL version.
pg_new_bindir: "{{ postgresql_bin_dir | regex_replace('(/$)', '') | replace(postgresql_version | string, pg_new_version | string) }}"
# Data directory path for the new PostgreSQL version.
pg_new_datadir: "{{ postgresql_data_dir | regex_replace('(/$)', '') | replace(postgresql_version | string, pg_new_version | string) }}"
# Configuration directory path for the new PostgreSQL version.
pg_new_confdir: "{{ postgresql_conf_dir | regex_replace('(/$)', '') | replace(postgresql_version | string, pg_new_version | string) }}"
# Custom WAL directory for the new PostgreSQL version (symlink will be created) [optional].
pg_new_wal_dir: "{{ postgresql_wal_dir | regex_replace('(/$)', '') | replace(postgresql_version | string, pg_new_version | string) }}"

# pg_upper_datadir: Specifies the top-level directory containing both old and new PostgreSQL data directories.
# The variable is derived from pg_new_datadir by removing any trailing slash and getting its grandparent directory.
# Adjust if the data directory location differs from the default.
# Example: /var/lib/postgresql, /var/lib/pgsql, /pgdata
pg_upper_datadir: "{{ pg_new_datadir | regex_replace('/$', '') | dirname | dirname }}"

# List of package names for the new PostgreSQL version to be installed.
# automatically detects the list of packages based on the 'postgresql_packages' variable
pg_new_packages: '{{ postgresql_packages | replace(postgresql_version | string, pg_new_version | string) }}'

# Alternatively, you can explicitly specify the list of new packages to install.
# This gives you more control and should be used if the automatic update does not meet your needs.
# Uncomment and modify the following lines according to your requirements. Example:
# pg_new_packages:
#  - postgresql-{{ pg_new_version }}
#  - postgresql-client-{{ pg_new_version }}
#  - postgresql-server-dev-{{ pg_new_version }}
#  - postgresql-contrib-{{ pg_new_version }}
#  - postgresql-{{ pg_new_version }}-repack"

pg_old_packages_remove: true # remove old postgresql packages after upgrade

# Timeout (in seconds) to be used when starting/stopping PostgreSQL during the upgrade.
pg_start_stop_timeout: 1800 # 30 minutes

# Patroni configuration file path.
patroni_config_file: /etc/patroni/patroni.yml

schema_compatibility_check: true # If 'true', a compatibility check of the database schema with the new PostgreSQL version will be performed before the upgrade.
schema_compatibility_check_port: '{{ (postgresql_port | int) + 1 }}' # Port used to run a temporary PostgreSQL instance for schema compatibility checking.
schema_compatibility_check_timeout: 3600 # Maximum duration (in seconds) for the compatibility check (using pg_dumpall --schema-only).

update_extensions: true # if 'true', try to update extensions automatically

vacuumdb_parallel_jobs: '{{ [ansible_processor_vcpus | int // 2, 1] | max }}' # use 50% CPU cores
vacuumdb_analyze_timeout: 3600 # seconds. The maximum duration of analyze command (soft limit, exceeding won't halt playbook)
# terminate active queries that are longer than the specified time (in seconds) during the collection of statistics.
vacuumdb_analyze_terminate_treshold: 0 # (0 = do not terminate active backends)

# Do not perform an upgrade if
max_replication_lag_bytes: 10485760 # 10 MiB - Maximum allowed replication lag in bytes
max_transaction_sec: 15 # Maximum allowed duration for a transactions in seconds

# (optional) Copy any files located in the "files" directory to all servers
# example for Postgres Full-Text Search (FTS) files
#  - { src: "files/numbers.syn", dest: "/usr/share/postgresql/{{ pg_new_version }}/tsearch_data/numbers.syn", owner: "root", group: "root", mode: "0644" }
#  - { src: "files/part_of_speech_russian.stop", dest: "/usr/share/postgresql/{{ pg_new_version }}/tsearch_data/part_of_speech_russian.stop", owner: "root", group: "root", mode: "0644" }
#  - { src: "files/ru_ru.affix", dest: "/usr/share/postgresql/{{ pg_new_version }}/tsearch_data/ru_ru.affix", owner: "root", group: "root", mode: "0644" }
#  - { src: "files/ru_ru.dict", dest: "/usr/share/postgresql/{{ pg_new_version }}/tsearch_data/ru_ru.dict", owner: "root", group: "root", mode: "0644" }

# if 'pgbouncer_install' is 'true'
pgbouncer_pool_pause: true # or 'false' if you don't want to pause pgbouncer pools during upgrade.
# the maximum waiting time (in seconds) for the pool to be paused. For each iteration of the loop when trying to pause all pools.
pgbouncer_pool_pause_timeout: 2
# the time (in seconds) after which instead of waiting for the completion of the active queries, the script terminates the slow active queries.
pgbouncer_pool_pause_terminate_after: 30
# the time (in seconds) after which the script exit with an error if it was not possible to pause all pgbouncer pools.
pgbouncer_pool_pause_stop_after: 60
# wait for the completion of active queries that are executed longer than the specified time (in milliseconds) before trying to pause the pool.
pg_slow_active_query_treshold: 1000
# terminate active queries that longer than the specified time (in milliseconds) after reaching "pgbouncer_pool_pause_terminate_after" before trying to pause the pool.
pg_slow_active_query_treshold_to_terminate: 100 # (0 = terminate all active backends)

# if 'pgbackrest_install' is 'true'
pgbackrest_stanza_upgrade: true # perform the "stanza-upgrade" command after the upgrade.
